{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class StochasticTwoStateMDP:\n",
    "    def __init__(self):\n",
    "        # two states: 0 and 1\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self, start_state=0):\n",
    "        \"\"\"Reset the environment to a given start state (default 0)\"\"\"\n",
    "        self.state = start_state\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Full transition dynamics for both actions:\n",
    "        \n",
    "        State 0:\n",
    "            Action D:\n",
    "                S = 0, R = 1 with P = 0.4\n",
    "                S = 1, R = 4 with P = 0.6\n",
    "            Action F:\n",
    "                S = 0, R = 3 with P = 0.4\n",
    "                S = 1, R = 2 with P = 0.6\n",
    "        State 1:\n",
    "            Action D:\n",
    "                S = 0, R = 1 with P = 0.7\n",
    "                S = 1, R = 4 with P = 0.3\n",
    "            Action F:\n",
    "                S = 0, R = 3 with P = 0.7\n",
    "                S = 1, R = 2 with P = 0.3\n",
    "        \"\"\"\n",
    "        s = self.state\n",
    "        p = random.random()\n",
    "\n",
    "        if s == 0:\n",
    "            if action == 'D':\n",
    "                if p < 0.4:\n",
    "                    next_state = 0\n",
    "                    r = 1\n",
    "                else:\n",
    "                    next_state = 1\n",
    "                    r = 4\n",
    "            elif action == 'F':\n",
    "                if p < 0.4:\n",
    "                    next_state = 0\n",
    "                    r = 3\n",
    "                else:\n",
    "                    next_state = 1\n",
    "                    r = 2\n",
    "            else:\n",
    "                raise ValueError(\"Invalid action\")\n",
    "        elif s == 1:\n",
    "            if action == 'D':\n",
    "                if p < 0.7:\n",
    "                    next_state = 0\n",
    "                    r = 1\n",
    "                else:\n",
    "                    next_state = 1\n",
    "                    r = 4\n",
    "            elif action == 'F':\n",
    "                if p < 0.7:\n",
    "                    next_state = 0\n",
    "                    r = 3\n",
    "                else:\n",
    "                    next_state = 1\n",
    "                    r = 2\n",
    "            else:\n",
    "                raise ValueError(\"Invalid action\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid state\")\n",
    "        \n",
    "        self.state = next_state\n",
    "        done = False\n",
    "        return next_state, r, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated state-value function V(s) for the policy DD (First-visit Monte Carlo):\n",
      "V(0) = 24.21\n",
      "V(1) = 23.59\n",
      "Estimated state-value function V(s) for the policy DF (First-visit Monte Carlo):\n",
      "V(0) = 27.59\n",
      "V(1) = 27.51\n",
      "Estimated state-value function V(s) for the policy FD (First-visit Monte Carlo):\n",
      "V(0) = 21.89\n",
      "V(1) = 21.44\n",
      "Estimated state-value function V(s) for the policy FF (First-visit Monte Carlo):\n",
      "V(0) = 25.26\n",
      "V(1) = 25.49\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_episodes = 1000\n",
    "max_steps = 100\n",
    "gamma = 0.9\n",
    "\n",
    "for i, title in enumerate([\"DD\", \"DF\", \"FD\", \"FF\"]):\n",
    "\n",
    "    def policy(state):\n",
    "        if state == 0:\n",
    "            return title[0]\n",
    "        elif state == 1:\n",
    "            return title[1]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid state\")\n",
    "        \n",
    "    returns_sum = {0: 0.0, 1: 0.0}\n",
    "    returns_count = {0: 0, 1: 0}\n",
    "    V = {0: 0.0, 1: 0.0}\n",
    "\n",
    "    V1_history = []\n",
    "    V2_history = []\n",
    "\n",
    "    env = StochasticTwoStateMDP()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        plce = returns_count.copy()\n",
    "        episode_list = []\n",
    "        state = env.reset(start_state=0)\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_list.append((state, reward))\n",
    "            state = next_state\n",
    "        \n",
    "        for i, (s, _) in enumerate(episode_list):\n",
    "            if returns_count[s] > plce[s]:\n",
    "                continue\n",
    "            G = 0.0\n",
    "            discount = 1.0\n",
    "            for j in range(i, len(episode_list)):\n",
    "                _, reward = episode_list[j]\n",
    "                G += discount * reward\n",
    "                discount *= gamma\n",
    "            returns_sum[s] += G\n",
    "            returns_count[s] += 1\n",
    "            V[s] = returns_sum[s] / returns_count[s]\n",
    "\n",
    "    print(f\"Estimated state-value function V(s) for the policy {title} (First-visit Monte Carlo):\")\n",
    "    print(f\"V(0) = {V[0]:.2f}\")\n",
    "    print(f\"V(1) = {V[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state = 0 -> action = D\n",
    "state = 1 -> action = F\n",
    "\n",
    "is the optimal policy, because the value function is the largest (~27.5) in either state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated state-value function V(s) for the policy DD (TD(0)):\n",
      "V(0) = 23.86\n",
      "V(1) = 24.02\n",
      "Estimated state-value function V(s) for the policy DF (TD(0)):\n",
      "V(0) = 27.61\n",
      "V(1) = 27.58\n",
      "Estimated state-value function V(s) for the policy FD (TD(0)):\n",
      "V(0) = 21.38\n",
      "V(1) = 21.41\n",
      "Estimated state-value function V(s) for the policy FF (TD(0)):\n",
      "V(0) = 25.46\n",
      "V(1) = 25.41\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "gamma = 0.9\n",
    "alpha = 0.01\n",
    "initial = 25.0\n",
    "\n",
    "for i, title in enumerate([\"DD\", \"DF\", \"FD\", \"FF\"]):\n",
    "\n",
    "    def policy(state):\n",
    "        if state == 0:\n",
    "            return title[0]\n",
    "        elif state == 1:\n",
    "            return title[1]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid state\")\n",
    "        \n",
    "    returns_sum = {0: 0.0, 1: 0.0}\n",
    "    returns_count = {0: 0, 1: 0}\n",
    "    V = {0: initial, 1: initial}\n",
    "\n",
    "    V1_history = []\n",
    "    V2_history = []\n",
    "\n",
    "    env = StochasticTwoStateMDP()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        episode_list = []\n",
    "        state = env.reset(start_state=0)\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_list.append((state, reward))\n",
    "            state = next_state\n",
    "        \n",
    "        for i, (s, _) in enumerate(episode_list):\n",
    "\n",
    "            target = 0.0\n",
    "            if i < len(episode_list) - 1:\n",
    "                s_next, reward = episode_list[i+1]\n",
    "                target = reward + gamma * V[s_next] - V[s]\n",
    "            returns_sum[s] += target * alpha\n",
    "            returns_count[s] += 1\n",
    "            V[s] = returns_sum[s] # * alpha\n",
    "\n",
    "    print(f\"Estimated state-value function V(s) for the policy {title} (TD(0)):\")\n",
    "    print(f\"V(0) = {V[0]:.2f}\")\n",
    "    print(f\"V(1) = {V[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state = 0 -> action = D\n",
    "state = 1 -> action = F\n",
    "\n",
    "is the optimal policy, because the value function is the largest (~27.6) in either state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class StochasticTwoStateMDP:\n",
    "    def __init__(self):\n",
    "        # two states: 0 and 1\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self, start_state=0):\n",
    "        \"\"\"Reset the environment to a given start state (default 0)\"\"\"\n",
    "        self.state = start_state\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Full transition dynamics for both actions:\n",
    "        \n",
    "        State 0:\n",
    "            Action D:\n",
    "                S = 0, R = 1 with P = 0.2\n",
    "                S = 1, R = 4 with P = 0.8\n",
    "            Action F:\n",
    "                S = 0, R = 3 with P = 0.6\n",
    "                S = 1, R = 2 with P = 0.4\n",
    "        State 1:\n",
    "            Action D:\n",
    "                S = 0, R = 1 with P = 0.1\n",
    "                S = 1, R = 4 with P = 0.9\n",
    "            Action F:\n",
    "                S = 0, R = 3 with P = 0.3\n",
    "                S = 1, R = 2 with P = 0.7\n",
    "        \"\"\"\n",
    "        s = self.state\n",
    "        p = random.random()\n",
    "\n",
    "        if s == 0:\n",
    "            if action == 'D':\n",
    "                if p < 0.2:\n",
    "                    next_state = 0\n",
    "                    r = 1\n",
    "                else:\n",
    "                    next_state = 1\n",
    "                    r = 4\n",
    "            elif action == 'F':\n",
    "                if p < 0.6:\n",
    "                    next_state = 0\n",
    "                    r = 3\n",
    "                else:\n",
    "                    next_state = 1\n",
    "                    r = 2\n",
    "            else:\n",
    "                raise ValueError(\"Invalid action\")\n",
    "        elif s == 1:\n",
    "            if action == 'D':\n",
    "                if p < 0.1:\n",
    "                    next_state = 0\n",
    "                    r = 1\n",
    "                else:\n",
    "                    next_state = 1\n",
    "                    r = 4\n",
    "            elif action == 'F':\n",
    "                if p < 0.3:\n",
    "                    next_state = 0\n",
    "                    r = 3\n",
    "                else:\n",
    "                    next_state = 1\n",
    "                    r = 2\n",
    "            else:\n",
    "                raise ValueError(\"Invalid action\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid state\")\n",
    "        \n",
    "        self.state = next_state\n",
    "        done = False\n",
    "        return next_state, r, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated state-value function V(s) for the policy DD (First-visit Monte Carlo):\n",
      "V(0) = 36.36\n",
      "V(1) = 36.67\n",
      "Estimated state-value function V(s) for the policy DF (First-visit Monte Carlo):\n",
      "V(0) = 26.74\n",
      "V(1) = 25.71\n",
      "Estimated state-value function V(s) for the policy FD (First-visit Monte Carlo):\n",
      "V(0) = 33.09\n",
      "V(1) = 35.05\n",
      "Estimated state-value function V(s) for the policy FF (First-visit Monte Carlo):\n",
      "V(0) = 24.52\n",
      "V(1) = 24.17\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_episodes = 1000\n",
    "max_steps = 100\n",
    "gamma = 0.9\n",
    "\n",
    "for i, title in enumerate([\"DD\", \"DF\", \"FD\", \"FF\"]):\n",
    "\n",
    "    def policy(state):\n",
    "        if state == 0:\n",
    "            return title[0]\n",
    "        elif state == 1:\n",
    "            return title[1]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid state\")\n",
    "        \n",
    "    returns_sum = {0: 0.0, 1: 0.0}\n",
    "    returns_count = {0: 0, 1: 0}\n",
    "    V = {0: 0.0, 1: 0.0}\n",
    "\n",
    "    V1_history = []\n",
    "    V2_history = []\n",
    "\n",
    "    env = StochasticTwoStateMDP()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        plce = returns_count.copy()\n",
    "        episode_list = []\n",
    "        state = env.reset(start_state=0)\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_list.append((state, reward))\n",
    "            state = next_state\n",
    "        \n",
    "        for i, (s, _) in enumerate(episode_list):\n",
    "            if returns_count[s] > plce[s]:\n",
    "                continue\n",
    "            G = 0.0\n",
    "            discount = 1.0\n",
    "            for j in range(i, len(episode_list)):\n",
    "                _, reward = episode_list[j]\n",
    "                G += discount * reward\n",
    "                discount *= gamma\n",
    "            returns_sum[s] += G\n",
    "            returns_count[s] += 1\n",
    "            V[s] = returns_sum[s] / returns_count[s]\n",
    "\n",
    "    print(f\"Estimated state-value function V(s) for the policy {title} (First-visit Monte Carlo):\")\n",
    "    print(f\"V(0) = {V[0]:.2f}\")\n",
    "    print(f\"V(1) = {V[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "always offering a discount is now the optimal policy, because the value function is highest (~36.5) regardless of state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated state-value function V(s) for the policy DD (TD(0)):\n",
      "V(0) = 36.88\n",
      "V(1) = 37.16\n",
      "Estimated state-value function V(s) for the policy DF (TD(0)):\n",
      "V(0) = 25.97\n",
      "V(1) = 25.88\n",
      "Estimated state-value function V(s) for the policy FD (TD(0)):\n",
      "V(0) = 33.95\n",
      "V(1) = 35.26\n",
      "Estimated state-value function V(s) for the policy FF (TD(0)):\n",
      "V(0) = 24.28\n",
      "V(1) = 24.17\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "gamma = 0.9\n",
    "alpha = 0.01\n",
    "initial = 25.0\n",
    "\n",
    "for i, title in enumerate([\"DD\", \"DF\", \"FD\", \"FF\"]):\n",
    "\n",
    "    def policy(state):\n",
    "        if state == 0:\n",
    "            return title[0]\n",
    "        elif state == 1:\n",
    "            return title[1]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid state\")\n",
    "        \n",
    "    returns_sum = {0: 0.0, 1: 0.0}\n",
    "    returns_count = {0: 0, 1: 0}\n",
    "    V = {0: initial, 1: initial}\n",
    "\n",
    "    V1_history = []\n",
    "    V2_history = []\n",
    "\n",
    "    env = StochasticTwoStateMDP()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        episode_list = []\n",
    "        state = env.reset(start_state=0)\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_list.append((state, reward))\n",
    "            state = next_state\n",
    "        \n",
    "        for i, (s, _) in enumerate(episode_list):\n",
    "\n",
    "            target = 0.0\n",
    "            if i < len(episode_list) - 1:\n",
    "                s_next, reward = episode_list[i+1]\n",
    "                target = reward + gamma * V[s_next] - V[s]\n",
    "            returns_sum[s] += target * alpha\n",
    "            returns_count[s] += 1\n",
    "            V[s] = returns_sum[s] # * alpha\n",
    "\n",
    "    print(f\"Estimated state-value function V(s) for the policy {title} (TD(0)):\")\n",
    "    print(f\"V(0) = {V[0]:.2f}\")\n",
    "    print(f\"V(1) = {V[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "always offering a discount is now the optimal policy, because the value function is highest (~37.0) regardless of state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/SrBM8AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA5kUlEQVR4nO3df3iU1Z3//9ckkJAAMwFDEiIJYGmhSEABi1kru9QskUZbBLfoUkv9URcMbhGrGKto211DsR/tj0XYfnvVuNcWUbdgKwhsLhBYS0REEQLKKg0NlUwgRWYgRALJ+f5BM5sJCUwmM5n7TJ6P65rrInOfue/3eZ8zM29m7nOPyxhjBAAAYJGEWAcAAADQWRQwAADAOhQwAADAOhQwAADAOhQwAADAOhQwAADAOhQwAADAOhQwAADAOr1iHUC0NDc368iRI+rfv79cLleswwEAACEwxujkyZPKzs5WQkLHn7PEbQFz5MgR5eTkxDoMAAAQhsOHD2vIkCEdbo/bAqZ///6SzifA7XbHOBoAABAKv9+vnJycwPt4R+K2gGn52sjtdlPAAABgmUud/sFJvAAAwDoUMAAAwDoUMAAAwDoUMAAAwDoUMAAAwDoUMAAAwDoUMAAAwDoUMAAAwDoUMAAAwDoUMAAAwDpx+1MC0fLwy7v18rufxDoMAAAc4dCSopgcl09gOuFzJesoXgAAaGXYI+ticlwKmBA9/PJuNZlYRwEAgPPEooihgAnRpgNHYx0CAAD4KwqYEN0wMiPWIQAAgL+igAnR0m9cpURXrKMAAMB5YnEiLwVMJxwsLdI3xl8e6zAAAHCMWK1Cchlj4vLUVL/fL4/HI5/PJ7fbHetwAABACEJ9/+YTGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYB0KGAAAYJ1OFTDLly/X2LFj5Xa75Xa7lZ+fr/Xr1we2f/bZZyouLtZll12mfv36aebMmaqtrQ3aR3V1tYqKipSamqqMjAw99NBDOnfuXFCbLVu2aPz48UpOTtaIESNUVlYWfg8BAEDc6VQBM2TIEC1ZskS7du3SO++8o6985Sv6+te/rn379kmSHnjgAb322mt65ZVXtHXrVh05ckQzZswIPL6pqUlFRUVqbGzU9u3b9cILL6isrEyLFy8OtKmqqlJRUZGmTJmi3bt3a8GCBbrnnnu0cePGCHUZAADYzmWMMV3ZwcCBA/X000/r1ltv1aBBg7Ry5UrdeuutkqQPP/xQX/ziF1VRUaFrr71W69ev10033aQjR44oMzNTkrRixQotWrRIx44dU1JSkhYtWqR169apsrIycIzbbrtNJ06c0IYNG0KOy+/3y+PxyOfzye12d6WLAACgm4T6/h32OTBNTU1atWqV6uvrlZ+fr127duns2bMqKCgItBk1apRyc3NVUVEhSaqoqFBeXl6geJGkwsJC+f3+wKc4FRUVQftoadOyj46cOXNGfr8/6AYAAOJTpwuYvXv3ql+/fkpOTtbcuXO1Zs0ajR49Wl6vV0lJSUpLSwtqn5mZKa/XK0nyer1BxUvL9pZtF2vj9/vV0NDQYVylpaXyeDyBW05OTme7BgAALNHpAmbkyJHavXu3duzYoXnz5mnOnDnav39/NGLrlJKSEvl8vsDt8OHDsQ4JAABESa/OPiApKUkjRoyQJE2YMEE7d+7Uz372M82aNUuNjY06ceJE0KcwtbW1ysrKkiRlZWXp7bffDtpfyyql1m3arlyqra2V2+1WSkpKh3ElJycrOTm5s90BAAAW6vJ1YJqbm3XmzBlNmDBBvXv31qZNmwLbDhw4oOrqauXn50uS8vPztXfvXh09ejTQpry8XG63W6NHjw60ab2PljYt+wAAAOjUJzAlJSWaNm2acnNzdfLkSa1cuVJbtmzRxo0b5fF4dPfdd2vhwoUaOHCg3G637r//fuXn5+vaa6+VJE2dOlWjR4/WHXfcoaVLl8rr9eqxxx5TcXFx4NOTuXPn6t/+7d/08MMP66677tLmzZv18ssva926dZHvPQAAsFKnCpijR4/qW9/6lmpqauTxeDR27Fht3LhRf//3fy9JevbZZ5WQkKCZM2fqzJkzKiws1HPPPRd4fGJiotauXat58+YpPz9fffv21Zw5c/TDH/4w0Gb48OFat26dHnjgAf3sZz/TkCFD9Ktf/UqFhYUR6jIAALBdl68D41RcBwYAAPtE/TowAAAAsUIBAwAArEMBAwAArEMBAwAArEMBAwAArEMBAwAArEMBAwAArEMBAwAArEMBAwAArEMBAwAArEMBAwAArEMBAwAArEMBAwAArEMBAwAArEMBAwAArEMBAwAArEMBAwAArEMBAwAArEMBAwAArEMBAwAArEMBY5kaX4O2H6xTja8h1qEAYYvEPO6O54ItcSI8No5N25jb60O0+uW0fPWKdQAI3Us7q1Wyeq+ajZTgkkpn5GnWNbmxDgvolEjM4+54LtgSJ8Jj49i0jfmWqy/Xmvc+CeqDpKj0y4n5chljTEwjiBK/3y+PxyOfzye32x3rcLqsxteg65ZsVnOr0Up0ufTmI1M02JMSu8CATojEPO6O54ItcSI8No5NezG3lSBJLkW8X92dr1Dfv/kKyRJVdfUXTNwmY3So7nRsAgLCEIl53B3PBVviRHhsHJv2Ym6rWYpKv5yaLwoYSwxP76sEV/B9iS6XhqWnxiYgIAyRmMfd8VywJU6Ex8axaS/mthKkqPTLqfmigLHEYE+KSmfkKdF1fhYlulx6asYYx37cCbQnEvO4O54LtsSJ8Ng4Nu3FPHP85UF/l87Mi0q/nJovzoGxTI2vQYfqTmtYemrMJw8QrkjM4+54LtgSJ8Jj49i0jbm9PkSrX92Vr1DfvylgAACAY3ASLwAAiFsUMAAAwDoUMAAAwDoUMAAAwDoUMAAAwDoUMAAAwDoUMAAAwDoUMAAAwDoUMAAAwDoUMAAAwDoUMAAAwDoUMAAAwDoUMAAAwDoUMAAAwDqdKmBKS0t1zTXXqH///srIyND06dN14MCBoDZ/93d/J5fLFXSbO3duUJvq6moVFRUpNTVVGRkZeuihh3Tu3LmgNlu2bNH48eOVnJysESNGqKysLLweAgCAuNOpAmbr1q0qLi7WW2+9pfLycp09e1ZTp05VfX19ULvvfOc7qqmpCdyWLl0a2NbU1KSioiI1NjZq+/bteuGFF1RWVqbFixcH2lRVVamoqEhTpkzR7t27tWDBAt1zzz3auHFjF7sLAADigcsYY8J98LFjx5SRkaGtW7dq8uTJks5/AnPVVVfppz/9abuPWb9+vW666SYdOXJEmZmZkqQVK1Zo0aJFOnbsmJKSkrRo0SKtW7dOlZWVgcfddtttOnHihDZs2BBSbH6/Xx6PRz6fT263O9wuAgCAbhTq+3eXzoHx+XySpIEDBwbd/5vf/Ebp6ekaM2aMSkpKdPr06cC2iooK5eXlBYoXSSosLJTf79e+ffsCbQoKCoL2WVhYqIqKig5jOXPmjPx+f9ANAADEp17hPrC5uVkLFizQddddpzFjxgTu/8d//EcNHTpU2dnZ2rNnjxYtWqQDBw5o9erVkiSv1xtUvEgK/O31ei/axu/3q6GhQSkpKRfEU1paqh/84AfhdgcAAFgk7AKmuLhYlZWVevPNN4Puv/feewP/zsvL0+DBg3XDDTfo4MGD+tznPhd+pJdQUlKihQsXBv72+/3KycmJ2vEAAEDshPUV0vz587V27Vq98cYbGjJkyEXbTpo0SZL08ccfS5KysrJUW1sb1Kbl76ysrIu2cbvd7X76IknJyclyu91BNwAAEJ86VcAYYzR//nytWbNGmzdv1vDhwy/5mN27d0uSBg8eLEnKz8/X3r17dfTo0UCb8vJyud1ujR49OtBm06ZNQfspLy9Xfn5+Z8IFAABxqlMFTHFxsf7zP/9TK1euVP/+/eX1euX1etXQ0CBJOnjwoH70ox9p165dOnTokH7/+9/rW9/6liZPnqyxY8dKkqZOnarRo0frjjvu0Pvvv6+NGzfqscceU3FxsZKTkyVJc+fO1R//+Ec9/PDD+vDDD/Xcc8/p5Zdf1gMPPBDh7gMAABt1ahm1y+Vq9/7nn39e3/72t3X48GF985vfVGVlperr65WTk6NbbrlFjz32WNBXOn/60580b948bdmyRX379tWcOXO0ZMkS9er1f6fkbNmyRQ888ID279+vIUOG6PHHH9e3v/3tkDvGMmoAAOwT6vt3l64D42QUMAAA2KdbrgMDAAAQCxQwAADAOhQwAADAOhQwAADAOhQwAADAOhQwAADAOhQwAADAOhQwAADAOhQwAADAOhQwAADAOhQwAADAOhQwAADAOhQwAADAOhQwAADAOhQwAADAOhQwAADAOhQwAADAOhQwAADAOhQwAADAOr1iHQCcocbXoKq6eg1P76vBnpRu29fF2kYyJgSzJbet4zzq/0xvHzquLw0bqHE5A7q8Pyf32xaXymfL9r5JiapvbOr21xcbxLo/sT5+V1DAQC/trFbJ6r1qNlKCSyqdkadZ1+RGfV8XaxvJmBDMlty2jrOtmeMv1//7xlVh78/J/bbFpfLZ3vh15+uLDWLdn1gfv6v4CqmHq/E1BL3INBvp0dWVqvE1RHVfF2sbyZgQzJbcto2zrd+++4neP/xp2Ptzar9tcal8djR+3fX6YoNY9yfWx48ECpgerqqu/oIXmSZjdKjudFT3dbG2kYwJwWzJbXtxtvXOodALGFv6bYtL5fNi49cdry82iHV/Yn38SOArpB5ueHpfJbgUNJETXS4NS0+N6r4u1TZSMSFYJMc7mtqLs62Jw0I/D8aWftviUvm82Ph1x+uLDWLdn1gfPxL4BKaHG+xJUemMPCW6XJLOT+CnZowJ62SuzuzrYm0jGROC2ZLbtnG2NXP85Z06kdeWftviUvnsaPy66/XFBrHuT6yPHwkuY8wlPqi1k9/vl8fjkc/nk9vtjnU4jlfja9ChutMalp4akVUCoe7rYm0jGROC2ZLb1nEe9X+mdw59qonDBnRpFZIN/bbFpfLZsj01KUGnG5u7/fXFBrHuT6yP355Q378pYAAAgGOE+v7NV0gAAMA6FDAAAMA6FDAAAMA6FDAAAMA6FDAAAMA6FDAAAMA6FDAAAMA6FDAAAMA6FDAAAMA6FDAAAMA6FDAAAMA6FDAAAMA6FDAAAMA6FDAAAMA6FDAAAMA6nSpgSktLdc0116h///7KyMjQ9OnTdeDAgaA2n332mYqLi3XZZZepX79+mjlzpmpra4PaVFdXq6ioSKmpqcrIyNBDDz2kc+fOBbXZsmWLxo8fr+TkZI0YMUJlZWXh9RAAAMSdThUwW7duVXFxsd566y2Vl5fr7Nmzmjp1qurr6wNtHnjgAb322mt65ZVXtHXrVh05ckQzZswIbG9qalJRUZEaGxu1fft2vfDCCyorK9PixYsDbaqqqlRUVKQpU6Zo9+7dWrBgge655x5t3LgxAl0GAAC2cxljTLgPPnbsmDIyMrR161ZNnjxZPp9PgwYN0sqVK3XrrbdKkj788EN98YtfVEVFha699lqtX79eN910k44cOaLMzExJ0ooVK7Ro0SIdO3ZMSUlJWrRokdatW6fKysrAsW677TadOHFCGzZsCCk2v98vj8cjn88nt9sdbhcBAEA3CvX9u0vnwPh8PknSwIEDJUm7du3S2bNnVVBQEGgzatQo5ebmqqKiQpJUUVGhvLy8QPEiSYWFhfL7/dq3b1+gTet9tLRp2Ud7zpw5I7/fH3QDAADxKewCprm5WQsWLNB1112nMWPGSJK8Xq+SkpKUlpYW1DYzM1NerzfQpnXx0rK9ZdvF2vj9fjU0NLQbT2lpqTweT+CWk5MTbtcAAIDDhV3AFBcXq7KyUqtWrYpkPGErKSmRz+cL3A4fPhzrkAAAQJT0CudB8+fP19q1a7Vt2zYNGTIkcH9WVpYaGxt14sSJoE9hamtrlZWVFWjz9ttvB+2vZZVS6zZtVy7V1tbK7XYrJSWl3ZiSk5OVnJwcTncAAIBlOvUJjDFG8+fP15o1a7R582YNHz48aPuECRPUu3dvbdq0KXDfgQMHVF1drfz8fElSfn6+9u7dq6NHjwbalJeXy+12a/To0YE2rffR0qZlHwAAoGfr1Cqk++67TytXrtTvfvc7jRw5MnC/x+MJfDIyb948vf766yorK5Pb7db9998vSdq+fbuk88uor7rqKmVnZ2vp0qXyer264447dM899+ipp56SdH4Z9ZgxY1RcXKy77rpLmzdv1j//8z9r3bp1KiwsDClWViEBAGCfkN+/TSdIavf2/PPPB9o0NDSY++67zwwYMMCkpqaaW265xdTU1ATt59ChQ2batGkmJSXFpKenmwcffNCcPXs2qM0bb7xhrrrqKpOUlGSuuOKKoGOEwufzGUnG5/N16nEAACB2Qn3/7tJ1YJyMT2AAALBPt1wHBgAAIBYoYAAAgHUoYAAAgHUoYAAAgHUoYAAAgHUoYAAAgHUoYAAAgHUoYAAAgHUoYAAAgHXC+jVqoCeo8TWoqq5ew9P7arCn/V9BB2zTMq/7JiWqvrGJ+d1DxcPrGwUM0I6XdlarZPVeNRspwSWVzsjTrGtyYx0W0CWt53UL5nfPEy+vb3yFBLRR42sIepFvNtKjqytV42uIbWBAF7Sd1y2Y3z1LPL2+UcAAbVTV1V/wIt9kjA7VnY5NQEAEtDevWzC/e454en2jgAHaGJ7eVwmu4PsSXS4NS0+NTUBABLQ3r1swv3uOeHp9o4AB2hjsSVHpjDwlus4/yxNdLj01Y4y1J7oB0oXzugXzu2eJp9c3lzGmgw8V7eb3++XxeOTz+eR2u2MdDixU42vQobrTGpaeauWTG2hPy7xOTUrQ6cZm5ncP5eTXt1Dfv1mFBHRgsCfFcU9soKuY15DiYx7wFRIAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALAOBQwAALBOpwuYbdu26eabb1Z2drZcLpdeffXVoO3f/va35XK5gm433nhjUJvjx49r9uzZcrvdSktL0913361Tp04FtdmzZ4+uv/569enTRzk5OVq6dGnnewcAAOJSpwuY+vp6jRs3TsuWLeuwzY033qiamprA7cUXXwzaPnv2bO3bt0/l5eVau3attm3bpnvvvTew3e/3a+rUqRo6dKh27dqlp59+Wk8++aR++ctfdjZcAAAQh3p19gHTpk3TtGnTLtomOTlZWVlZ7W774IMPtGHDBu3cuVMTJ06UJP3iF7/QV7/6Vf3kJz9Rdna2fvOb36ixsVG//vWvlZSUpCuvvFK7d+/WM888E1ToILZqfA2qqqvX8PS+GuxJiXU4cJCuzo1IzS0nzNFwY4hG7E7IR0c6iq0rMbf32GgcB7HR6QImFFu2bFFGRoYGDBigr3zlK/qXf/kXXXbZZZKkiooKpaWlBYoXSSooKFBCQoJ27NihW265RRUVFZo8ebKSkpICbQoLC/XjH/9Yn376qQYMGBCNsNEJL+2sVsnqvWo2UoJLKp2Rp1nX5MY6LDhAV+dGpOaWE+ZouDFEI3Yn5KMjHcXWlZjbe6ykiB8HsRPxk3hvvPFG/cd//Ic2bdqkH//4x9q6daumTZumpqYmSZLX61VGRkbQY3r16qWBAwfK6/UG2mRmZga1afm7pU1bZ86ckd/vD7ohOmp8DYEnuyQ1G+nR1ZWq8TXENjDEXFfnRqTmlhPmaLgxRCN2J+SjIx3F9v7hT8OOub19lvx2b8SPg9iKeAFz22236Wtf+5ry8vI0ffp0rV27Vjt37tSWLVsifaggpaWl8ng8gVtOTk5Uj9eTVdXVB57sLZqM0aG607EJCI7R1bkRqbnlhDkabgzRiN0J+ehIR7HtPPRp2DG3t89mKeLHQWxFfRn1FVdcofT0dH388ceSpKysLB09ejSozblz53T8+PHAeTNZWVmqra0NatPyd0fn1pSUlMjn8wVuhw8fjnRX8FfD0/sqwRV8X6LLpWHpqbEJCI7R1bkRqbnlhDkabgzRiN0J+ehIR7FdM2xA2DG3t88EKeLHQWxFvYD585//rL/85S8aPHiwJCk/P18nTpzQrl27Am02b96s5uZmTZo0KdBm27ZtOnv2bKBNeXm5Ro4c2eH5L8nJyXK73UE3RMdgT4pKZ+Qp0XX+WZ/ocumpGWM48Q1dnhuRmltOmKPhxhCN2J2Qj450FNu4nAFhx9zePktn5kX8OIgtlzHGXLrZ/zl16lTg05Srr75azzzzjKZMmaKBAwdq4MCB+sEPfqCZM2cqKytLBw8e1MMPP6yTJ09q7969Sk5OlnR+JVNtba1WrFihs2fP6s4779TEiRO1cuVKSZLP59PIkSM1depULVq0SJWVlbrrrrv07LPPhrwKye/3y+PxyOfzUcxESY2vQYfqTmtYeipPdgTp6tyI1NxywhwNN4ZoxO6EfHSko9i6EnN7j43GcRBZob5/d7qA2bJli6ZMmXLB/XPmzNHy5cs1ffp0vffeezpx4oSys7M1depU/ehHPwo6Kff48eOaP3++XnvtNSUkJGjmzJn6+c9/rn79+gXa7NmzR8XFxdq5c6fS09N1//33a9GiRSHHSQEDAIB9olbA2IICBgAA+4T6/s1vIQEAAOtQwAAAAOtQwAAAAOtQwAAAAOtQwAAAAOtQwAAAAOtQwAAAAOtQwAAAAOtQwAAAAOtQwAAAAOtQwAAAAOtQwAAAAOtQwAAAAOtQwAAAAOtQwAAAAOtQwAAAAOtQwAAAAOtQwAAAAOtQwAAAAOtQwAAAAOtQwACWqvE1aPvBOtX4GmIdStzrTK4jMS42j63NsbcWL/2IZ71iHQCAzntpZ7VKVu9Vs5ESXFLpjDzNuiY31mHFpc7kOhLjYvPY2hx7a/HSj3jHJzCAZWp8DYEXV0lqNtKjqyv5n2IUdCbXkRgXm8fW5thbi5d+9AQUMIBlqurqAy+uLZqM0aG607EJKI51JteRGBebx9bm2FuLl370BBQwgGWGp/dVgiv4vkSXS8PSU2MTUBzrTK4jMS42j63NsbcWL/3oCShgAMsM9qSodEaeEl3nX2UTXS49NWOMBntSYhxZ/OlMriMxLjaPrc2xtxYv/egJXMYYc+lm9vH7/fJ4PPL5fHK73bEOB4i4Gl+DDtWd1rD0VF5co6wzuY7EuNg8tjbH3lq89MNGob5/U8AAAADHCPX9m6+QAACAdShgAACAdShgAACAdShgAACAdShgAACAdShgAACAdShgAACAdShgAACAdShgAACAdShgAACAdShgAACAdShgAACAdShgAACAdShgAACAdTpdwGzbtk0333yzsrOz5XK59OqrrwZtN8Zo8eLFGjx4sFJSUlRQUKCPPvooqM3x48c1e/Zsud1upaWl6e6779apU6eC2uzZs0fXX3+9+vTpo5ycHC1durTzvQMAAHGp0wVMfX29xo0bp2XLlrW7fenSpfr5z3+uFStWaMeOHerbt68KCwv12WefBdrMnj1b+/btU3l5udauXatt27bp3nvvDWz3+/2aOnWqhg4dql27dunpp5/Wk08+qV/+8pdhdBHxpsbXoO0H61Tja4h1KBEVr/2yjVPGwSlxdAen9dVp8URTZ/rqtLy4jDEm7Ae7XFqzZo2mT58u6fynL9nZ2XrwwQf1ve99T5Lk8/mUmZmpsrIy3Xbbbfrggw80evRo7dy5UxMnTpQkbdiwQV/96lf15z//WdnZ2Vq+fLm+//3vy+v1KikpSZL0yCOP6NVXX9WHH34YUmx+v18ej0c+n09utzvcLsJhXtpZrZLVe9VspASXVDojT7OuyY11WF0Wr/2yjVPGwSlxdAen9dVp8URTZ/ranXkJ9f07oufAVFVVyev1qqCgIHCfx+PRpEmTVFFRIUmqqKhQWlpaoHiRpIKCAiUkJGjHjh2BNpMnTw4UL5JUWFioAwcO6NNPP2332GfOnJHf7w+6Ib7U+BoCTyBJajbSo6srHfO/gXDFa79s45RxcEoc3cFpfXVaPNHUmb46NS8RLWC8Xq8kKTMzM+j+zMzMwDav16uMjIyg7b169dLAgQOD2rS3j9bHaKu0tFQejydwy8nJ6XqH4ChVdfWBJ1CLJmN0qO50bAKKkHjtl22cMg5OiaM7OK2vTosnmjrTV6fmJW5WIZWUlMjn8wVuhw8fjnVIiLDh6X2V4Aq+L9Hl0rD01NgEFCHx2i/bOGUcnBJHd3BaX50WTzR1pq9OzUtEC5isrCxJUm1tbdD9tbW1gW1ZWVk6evRo0PZz587p+PHjQW3a20frY7SVnJwst9sddEN8GexJUemMPCW6zj+TEl0uPTVjjAZ7UmIcWdfEa79s45RxcEoc3cFpfXVaPNHUmb46Ni+mCySZNWvWBP5ubm42WVlZ5ic/+UngPp/PZ5KTk82LL75ojDFm//79RpJ55513Am02btxoXC6X+eSTT4wxxjz33HNmwIABprGxMdCmpKTEjBw5MuTYfD6fkWR8Pl+43YNDHTlx2mz/uM4cOXE61qFEVLz2yzZOGQenxNEdnNZXp8UTTZ3pa3flJdT3706vQjp16pQ+/vhjSdLVV1+tZ555RlOmTNHAgQOVm5urH//4x1qyZIleeOEFDR8+XI8//rj27Nmj/fv3q0+fPpKkadOmqba2VitWrNDZs2d15513auLEiVq5cqWk8yuXRo4cqalTp2rRokWqrKzUXXfdpWeffTZoufXFsAoJAAD7hPz+3dnK6I033jCSLrjNmTPHGHP+U5jHH3/cZGZmmuTkZHPDDTeYAwcOBO3jL3/5i7n99ttNv379jNvtNnfeeac5efJkUJv333/ffPnLXzbJycnm8ssvN0uWLOlUnHwCAwCAfaL2CYwt+AQGAAD7xOQ6MAAAAN2BAgYAAFiHAgYAAFiHAgYAAFiHAgYAAFiHAgYAAFiHAgYAAFiHAgYAAFiHAgYAAFinV6wDiFc1vgZV1dVreHrf2P9iZwQ4sT+xjKntsZ2Yn3jn9JzHKr6W4/ZNSlR9Y1OXjh+JPjh9nGAvCpgoeGlntUpW71WzkRJcUumMPM26JjfWYYXNif2JZUxtj33L1ZdrzXufOCo/8c6Jc7K1WMXX+rgtwj1+JPrg9HGC3fgKKcJqfA1BLyDNRnp0daVqfA2xDSxMTuxPLGNq79i/ffcTR+Un3jlxTrYWq/jaHrdFOMePRB+cPk6wHwVMhFXV1V/wAtJkjA7VnY5NQF3kxP7EMqb2jt1WrPMT75w4J1uLVXwXm5udPX4k+uD0cYL9KGAibHh6XyW4gu9LdLk0LD01NgF1kRP7E8uY2jt2W7HOT7xz4pxsLVbxXWxudvb4keiD08cJ9qOAibDBnhSVzshTouv8MzfR5dJTM8ZYe/KaE/sTy5jaO/bM8Zc7Kj/xzolzsrVYxdf2uC3COX4k+uD0cYL9XMaYS3wgbie/3y+PxyOfzye3293tx6/xNehQ3WkNS0+NiyesE/sTy5jaHtuJ+Yl3Ts95rOJrOW5qUoJONzZ36fiR6IPTxwnOE+r7NwUMAABwjFDfv/kKCQAAWIcCBgAAWIcCBgAAWIcCBgAAWIcCBgAAWIcCBgAAWIcCBgAAWIcCBgAAWIcCpoer8TVo+8E6fiHWgcIdG8a0Z2Lc4w9jenG9Yh0AYuelndWBn7tPcEmlM/I065rcWIcFhT82jGnPxLjHH8b00vgEpoeq8TUEnhyS1GykR1dXUuk7QLhjw5j2TIx7/GFMQ0MB00NV1dUHnhwtmozRobrTsQkIAeGODWPaMzHu8YcxDQ0FTA81PL2vElzB9yW6XBqWnhqbgBAQ7tgwpj0T4x5/GNPQUMD0UIM9KSqdkadE1/lnSaLLpadmjOHn7h0g3LFhTHsmxj3+MKahcRljzKWb2SfUn+Pu6Wp8DTpUd1rD0lN5cjhMuGPDmPZMjHv86aljGur7NwUMAABwjFDfv/kKCQAAWIcCBgAAWIcCBgAAWIcCBgAAWIcCBgAAWIcCBlaz/cfObI8/XD2137HS1Xy3fbzTxs9p8URavPcvXPyYI6xl+4+d2R5/uHpqv2Olq/lu+/hbrr5ca977xDHjF+/zKd771xV8AgMr2f5jZ7bHH66e2u9Y6Wq+23v8b9/9xDHjF+/zKd7711UUMLCS7T92Znv84eqp/Y6Vrua7vce3Fcvxi/f5FO/966qIFzBPPvmkXC5X0G3UqFGB7Z999pmKi4t12WWXqV+/fpo5c6Zqa2uD9lFdXa2ioiKlpqYqIyNDDz30kM6dOxfpUGEx23/szPb4w9VT+x0rXc13e49vK5bjF+/zKd7711VR+QTmyiuvVE1NTeD25ptvBrY98MADeu211/TKK69o69atOnLkiGbMmBHY3tTUpKKiIjU2Nmr79u164YUXVFZWpsWLF0cjVFjK9h87sz3+cPXUfsdKV/Pd3uNnjr/cMeMX7/Mp3vvXVRH/LaQnn3xSr776qnbv3n3BNp/Pp0GDBmnlypW69dZbJUkffvihvvjFL6qiokLXXnut1q9fr5tuuklHjhxRZmamJGnFihVatGiRjh07pqSkpJDi4LeQegbbf+zM9vjD1VP7HStdzXfbxztt/JwWT6TFe//aCvX9OyqrkD766CNlZ2erT58+ys/PV2lpqXJzc7Vr1y6dPXtWBQUFgbajRo1Sbm5uoICpqKhQXl5eoHiRpMLCQs2bN0/79u3T1Vdf3e4xz5w5ozNnzgT+9vv90egaHGawJ8XqJ7Tt8Yerp/Y7Vrqa77aPd9r4OS2eSIv3/oUr4l8hTZo0SWVlZdqwYYOWL1+uqqoqXX/99Tp58qS8Xq+SkpKUlpYW9JjMzEx5vV5JktfrDSpeWra3bOtIaWmpPB5P4JaTkxPZjgEAAMeI+Ccw06ZNC/x77NixmjRpkoYOHaqXX35ZKSnRqyBLSkq0cOHCwN9+v58iBgCAOBX1ZdRpaWn6whe+oI8//lhZWVlqbGzUiRMngtrU1tYqKytLkpSVlXXBqqSWv1vatCc5OVlutzvohtjq6OqR4V5VMpZXo+RKmPGtZXzfP/xpjx9n5np4uiNvjE2wqF+J99SpUzp48KDuuOMOTZgwQb1799amTZs0c+ZMSdKBAwdUXV2t/Px8SVJ+fr7+9V//VUePHlVGRoYkqby8XG63W6NHj452uIiQjq4eGe5VJWN5NUquhBnfWo9vi546zsz18HRH3hibC0V8FdL3vvc93XzzzRo6dKiOHDmiJ554Qrt379b+/fs1aNAgzZs3T6+//rrKysrkdrt1//33S5K2b98u6fwy6quuukrZ2dlaunSpvF6v7rjjDt1zzz166qmnQo6DVUixU+Nr0HVLNge9ISS6XFp9X75ueW77Bfe/+ciUi56g1tH+LvW4SIjlsRF97Y1vi542zsz18HRH3nra2IT6/h3xr5D+/Oc/6/bbb9fIkSP1jW98Q5dddpneeustDRo0SJL07LPP6qabbtLMmTM1efJkZWVlafXq1YHHJyYmau3atUpMTFR+fr6++c1v6lvf+pZ++MMfRjpURElHV4/ceejTsK4qGcurUXIlzPh2sSvN9rRxZq6Hpzvyxti0L+JfIa1ateqi2/v06aNly5Zp2bJlHbYZOnSoXn/99UiHhm7ScvXItv9buGbYgHbvv9RVJTvaX3dcjTKWx0b0tTe+LXraODPXw9MdeWNs2sdvISHiOrp65LicAWFdVTKWV6PkSpjxre34tuiJ48xcD0935I2xaV/Ez4FxCs6Bib2Orh4Z7lUlY3k1yp52JcyepmV8U5MSdLqxuUePM3M9PN2Rt54yNqG+f1PAAAAAx4jZSbwAAADRRgHjcNG4cFFPuxiSU/sbalyRjr+78tH2OMxlRJoTx9+JMcWrqF/IDuGLxoWLetrFkJza31DjinT83ZWPtse55erLtea9T5jLiBgnjr8TY4pnfALjUDW+hqCrgzYb6dHVlV2q6qOxTydzan9DjSvS8XdXPto7zm/f/YS5jIhx4vg7MaZ4RwHjUNG4cFFPuxiSU/sbalyRjr+78nGxi8NF6rhOHVt0DyeOvxNjincUMA7VcuGi1rp64aJo7NPJnNrfUOOKdPzdlY/2jtMWcxld4cTxd2JM8Y4CxqGiceGinnYxJKf2N9S4Ih1/d+WjvePMHH85cxkR48Txd2JM8Y7rwDhcNC5c1FMuhtTCqf0NNa5Ix99d+Wh7HOYyIs2J4+/EmGzDhewcWMDU+Br0zqHjcrlcyhmQovrGJg1P78skj4AaX4Oq6uo1PL2vJAXyPGHogMCbZ8t28o22ojk/2s5N5mHn8NzteUJ9/2YZdTd5aWe1HvntXrWtFllq13Wtly66pKAcuyTNGB/5JbyIH9Fc+tp2bkrn5yfzMDQsS8bFcA5MN6jxNbRbvEgsteuqtksX2+bYKPJLeBE/orn0tb252TI/mYeXxrJkXAoFTDeoqqtvt3hpwVK78IWyZLct8o0W0Vz6eqm5yTy8OJYl41IoYLrB8PS+utiqUpbahS+UJbttkW+0iObS10vNTebhxbEsGZdCAdMNBntStGRmXrtFDEvtuqbt0sW2OXa5FPElvIgf0Vz62t7c/Os/mYchYFkyLoVVSN2oxtegXYc+lcslDRmQotONzSy166KWFQp9kxID+ZQUyPP4VquQWpY2SqwEiWfhrFqJ5tLXtnOvu5bYRnr1TqxWA/WUZcmstvo/LKN2YAGDyApnhQKrGuIb43uerT8C2lOR32Chvn/zFRKsFM4KBVY1xDfG9zxbfwS0pyK/4aOAgZXCWaHAqob4xvieZ+uPgPZU5Dd8XMgOVmpZodD6iX+pFQrhPCZcfJ/d/bpzfNvT+nysWF5lO9J5iHVe410o+eX1pH18AgMrhbNCobtWNby0s1rXLdmsf/z/dui6JZv10s7qiO4f7YvlqpXWY/71ZdtjOva2/ghoT3Wp/PJ60jFO4oXVwlmhEO0VJ9ct2XzB/6befGQKL/jdpLtXrbQ35i1iOfa2/ghoT9Vefnvq6wm/hYQeYbAnpdNP5HAeE6qLfZ8dzy84ThLN8W3Pxa64G8uxj3QeujuvPU17+eX15OL4CqmHqPE1aPvBOs5sb0ckc3Oxq4fG2xh0pj/x1vfWLnbFXc4VsYvT5ml7cytB0l/qz0Ts97qc1N/O4iukHoBrDHQsGrl5aWe1Hl1dqSZjAt9nS4qrMehM3nrC/Gs95i1axj7e+hqvnDpPW8+tSP6iuVP7K3EhOwqYv+qp36GGIpq5aXv11Xgag87krSfNv5YxT01K4CrblnH6PG25ivs/r3ovIjE6vb8UMFEqYF55p1oP/dfeiO0PAACbHVpSFNH9cSXeKJi8dDPFCwAArQx7ZF1MjksBE6JX3qlW9XE7T3QCACCaYlHEUMCEaEOlN9YhAACAv6KACdGNY7JiHQIAAPgrCpgQ/cPEXOUOjP3Z2QAAOE2kT+QNBQVMJ2x7+Ct6+ta8WIcBAIBjxKJ4kfgpgU77h4m5+oeJzrjYDwAAPRWfwAAAAOtQwAAAAOtQwAAAAOtQwAAAAOtQwAAAAOtQwAAAAOs4uoBZtmyZhg0bpj59+mjSpEl6++23Yx0SAABwAMcWMC+99JIWLlyoJ554Qu+++67GjRunwsJCHT16NNahAQCAGHNsAfPMM8/oO9/5ju68806NHj1aK1asUGpqqn7961/HOjQAABBjjixgGhsbtWvXLhUUFATuS0hIUEFBgSoqKtp9zJkzZ+T3+4NuAAAgPjmygKmrq1NTU5MyMzOD7s/MzJTX6233MaWlpfJ4PIFbTk5Od4QKAABiIG5+C6mkpEQLFy4M/O3z+ZSbm8snMQAAWKTlfdsYc9F2jixg0tPTlZiYqNra2qD7a2trlZWV1e5jkpOTlZycHPi7JQF8EgMAgH1Onjwpj8fT4XZHFjBJSUmaMGGCNm3apOnTp0uSmpubtWnTJs2fPz+kfWRnZ+vw4cPq37+/XC5XxGLz+/3KycnR4cOH5Xa7I7ZfXIhcdw/y3D3Ic/cgz90jmnk2xujkyZPKzs6+aDtHFjCStHDhQs2ZM0cTJ07Ul770Jf30pz9VfX297rzzzpAen5CQoCFDhkQtPrfbzZOjm5Dr7kGeuwd57h7kuXtEK88X++SlhWMLmFmzZunYsWNavHixvF6vrrrqKm3YsOGCE3sBAEDP49gCRpLmz58f8ldGAACg53DkMmonS05O1hNPPBF0wjCig1x3D/LcPchz9yDP3cMJeXaZS61TAgAAcBg+gQEAANahgAEAANahgAEAANahgAEAANahgOmkZcuWadiwYerTp48mTZqkt99+O9YhOca2bdt08803Kzs7Wy6XS6+++mrQdmOMFi9erMGDByslJUUFBQX66KOPgtocP35cs2fPltvtVlpamu6++26dOnUqqM2ePXt0/fXXq0+fPsrJydHSpUsviOWVV17RqFGj1KdPH+Xl5en111+PeH9jpbS0VNdcc4369++vjIwMTZ8+XQcOHAhq89lnn6m4uFiXXXaZ+vXrp5kzZ17w0xzV1dUqKipSamqqMjIy9NBDD+ncuXNBbbZs2aLx48crOTlZI0aMUFlZ2QXxxOtzYvny5Ro7dmzgQl35+flav359YDs5jo4lS5bI5XJpwYIFgfvIddc9+eSTcrlcQbdRo0YFtluZY4OQrVq1yiQlJZlf//rXZt++feY73/mOSUtLM7W1tbEOzRFef/118/3vf9+sXr3aSDJr1qwJ2r5kyRLj8XjMq6++at5//33zta99zQwfPtw0NDQE2tx4441m3Lhx5q233jL/8z//Y0aMGGFuv/32wHafz2cyMzPN7NmzTWVlpXnxxRdNSkqK+fd///dAmz/84Q8mMTHRLF261Ozfv9889thjpnfv3mbv3r1Rz0F3KCwsNM8//7yprKw0u3fvNl/96ldNbm6uOXXqVKDN3LlzTU5Ojtm0aZN55513zLXXXmv+5m/+JrD93LlzZsyYMaagoMC899575vXXXzfp6emmpKQk0OaPf/yjSU1NNQsXLjT79+83v/jFL0xiYqLZsGFDoE08Pyd+//vfm3Xr1pn//d//NQcOHDCPPvqo6d27t6msrDTGkONoePvtt82wYcPM2LFjzXe/+93A/eS665544glz5ZVXmpqamsDt2LFjge025pgCphO+9KUvmeLi4sDfTU1NJjs725SWlsYwKmdqW8A0NzebrKws8/TTTwfuO3HihElOTjYvvviiMcaY/fv3G0lm586dgTbr1683LpfLfPLJJ8YYY5577jkzYMAAc+bMmUCbRYsWmZEjRwb+/sY3vmGKioqC4pk0aZL5p3/6p4j20SmOHj1qJJmtW7caY87ntXfv3uaVV14JtPnggw+MJFNRUWGMOV9sJiQkGK/XG2izfPly43a7A7l9+OGHzZVXXhl0rFmzZpnCwsLA3z3tOTFgwADzq1/9ihxHwcmTJ83nP/95U15ebv72b/82UMCQ68h44oknzLhx49rdZmuO+QopRI2Njdq1a5cKCgoC9yUkJKigoEAVFRUxjMwOVVVV8nq9QfnzeDyaNGlSIH8VFRVKS0vTxIkTA20KCgqUkJCgHTt2BNpMnjxZSUlJgTaFhYU6cOCAPv3000Cb1sdpaROv4+Tz+SRJAwcOlCTt2rVLZ8+eDcrBqFGjlJubG5TrvLy8oJ/mKCwslN/v1759+wJtLpbHnvScaGpq0qpVq1RfX6/8/HxyHAXFxcUqKiq6IB/kOnI++ugjZWdn64orrtDs2bNVXV0tyd4cU8CEqK6uTk1NTRf8FlNmZqa8Xm+MorJHS44ulj+v16uMjIyg7b169dLAgQOD2rS3j9bH6KhNPI5Tc3OzFixYoOuuu05jxoyRdL7/SUlJSktLC2rbNtfh5tHv96uhoaFHPCf27t2rfv36KTk5WXPnztWaNWs0evRochxhq1at0rvvvqvS0tILtpHryJg0aZLKysq0YcMGLV++XFVVVbr++ut18uRJa3Ps6N9CAnBxxcXFqqys1JtvvhnrUOLSyJEjtXv3bvl8Pv3Xf/2X5syZo61bt8Y6rLhy+PBhffe731V5ebn69OkT63Di1rRp0wL/Hjt2rCZNmqShQ4fq5ZdfVkpKSgwjCx+fwIQoPT1diYmJF5yVXVtbq6ysrBhFZY+WHF0sf1lZWTp69GjQ9nPnzun48eNBbdrbR+tjdNQm3sZp/vz5Wrt2rd544w0NGTIkcH9WVpYaGxt14sSJoPZtcx1uHt1ut1JSUnrEcyIpKUkjRozQhAkTVFpaqnHjxulnP/sZOY6gXbt26ejRoxo/frx69eqlXr16aevWrfr5z3+uXr16KTMzk1xHQVpamr7whS/o448/tnY+U8CEKCkpSRMmTNCmTZsC9zU3N2vTpk3Kz8+PYWR2GD58uLKysoLy5/f7tWPHjkD+8vPzdeLECe3atSvQZvPmzWpubtakSZMCbbZt26azZ88G2pSXl2vkyJEaMGBAoE3r47S0iZdxMsZo/vz5WrNmjTZv3qzhw4cHbZ8wYYJ69+4dlIMDBw6ouro6KNd79+4NKhjLy8vldrs1evToQJuL5bEnPieam5t15swZchxBN9xwg/bu3avdu3cHbhMnTtTs2bMD/ybXkXfq1CkdPHhQgwcPtnc+d/q03x5s1apVJjk52ZSVlZn9+/ebe++916SlpQWdld2TnTx50rz33nvmvffeM5LMM888Y9577z3zpz/9yRhzfhl1Wlqa+d3vfmf27Nljvv71r7e7jPrqq682O3bsMG+++ab5/Oc/H7SM+sSJEyYzM9PccccdprKy0qxatcqkpqZesIy6V69e5ic/+Yn54IMPzBNPPBFXy6jnzZtnPB6P2bJlS9CSyNOnTwfazJ071+Tm5prNmzebd955x+Tn55v8/PzA9pYlkVOnTjW7d+82GzZsMIMGDWp3SeRDDz1kPvjgA7Ns2bJ2l0TG63PikUceMVu3bjVVVVVmz5495pFHHjEul8v893//tzGGHEdT61VIxpDrSHjwwQfNli1bTFVVlfnDH/5gCgoKTHp6ujl69Kgxxs4cU8B00i9+8QuTm5trkpKSzJe+9CXz1ltvxTokx3jjjTeMpAtuc+bMMcacX0r9+OOPm8zMTJOcnGxuuOEGc+DAgaB9/OUvfzG333676devn3G73ebOO+80J0+eDGrz/vvvmy9/+csmOTnZXH755WbJkiUXxPLyyy+bL3zhCyYpKclceeWVZt26dVHrd3drL8eSzPPPPx9o09DQYO677z4zYMAAk5qaam655RZTU1MTtJ9Dhw6ZadOmmZSUFJOenm4efPBBc/bs2aA2b7zxhrnqqqtMUlKSueKKK4KO0SJenxN33XWXGTp0qElKSjKDBg0yN9xwQ6B4MYYcR1PbAoZcd92sWbPM4MGDTVJSkrn88svNrFmzzMcffxzYbmOOXcYY0/nPbQAAAGKHc2AAAIB1KGAAAIB1KGAAAIB1KGAAAIB1KGAAAIB1KGAAAIB1KGAAAIB1KGAAAIB1KGAAAIB1KGAAAIB1KGAAAIB1KGAAAIB1/n8+PPx9cTdMPQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E(G_0) = 2039.0240829609654\n",
      "95% Confidence Interval: (2025.3453254162707, 2052.70284050566)\n"
     ]
    }
   ],
   "source": [
    "# part a\n",
    "import random\n",
    "import math\n",
    "import scipy.stats as st\n",
    "\n",
    "actions = [0, 0.5, 1]\n",
    "S = [1000*math.e**(0.2 * k) for k in range(-10, 7)]\n",
    "gamma = 0.9\n",
    "A = 1 # always choosing A = 1\n",
    "\n",
    "def epsilon():\n",
    "    return random.random() * 2.05 - 0.85\n",
    "\n",
    "def snap(s):\n",
    "    return S[np.argmin(abs(np.subtract(S, s)))]\n",
    "\n",
    "E = []\n",
    "for i in range(50000): # 50,000 iterations\n",
    "    w = snap(1000)\n",
    "    k = 0\n",
    "    while(w > 150 and w < 3000 and k < 40):\n",
    "        w *= 1 + epsilon() * A\n",
    "        w = snap(w)\n",
    "        k += 1\n",
    "\n",
    "    E.append(w)\n",
    "\n",
    "plt.scatter(range(len(E)), E, marker='.')\n",
    "plt.show()\n",
    "mean = np.mean(E)\n",
    "stddev = np.std(E, ddof=1)\n",
    "\n",
    "se = stddev / np.sqrt(len(E))\n",
    "\n",
    "ci = st.t.interval(0.95, len(E) - 1, loc=mean, scale=se)\n",
    "print(\"E(G_0) =\", mean)\n",
    "print(\"95% Confidence Interval:\", ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 401.33990192    0.            0.        ]\n",
      " [ 226.02611236  231.12921896  461.33754127]\n",
      " [ 224.48266916  266.9075852   502.21466373]\n",
      " [ 324.35352847  467.72730421  365.33610289]\n",
      " [ 410.45966636  386.43196507  628.15267249]\n",
      " [ 481.25870277  484.76961435  648.2402177 ]\n",
      " [ 516.28224649  523.96605477  732.96570703]\n",
      " [ 593.94990234  587.1426111   778.2593137 ]\n",
      " [ 612.09632209  612.63936492  979.20742437]\n",
      " [ 812.11873515 1093.67716895  784.32531708]\n",
      " [ 959.7590321  1027.97577933 1492.56743296]\n",
      " [1052.99341662 1639.52020908 1097.98374621]\n",
      " [1307.65124184 1206.3105897  1999.75915379]\n",
      " [1625.13297177 2263.49965063 1818.96034542]\n",
      " [1933.22429357 2628.24329495 1760.14272059]\n",
      " [2247.64562273 2250.7862253  2770.54182174]\n",
      " [   0.            0.            0.        ]]\n",
      "[0.  1.  1.  0.5 1.  1.  1.  1.  1.  0.5 1.  0.5 1.  0.5 0.5 1.  0. ]\n",
      "E(G_0) = 2581.799426978949\n",
      "95% Confidence Interval: (2554.4736030540707, 2609.1252509038277)\n"
     ]
    }
   ],
   "source": [
    "# part b\n",
    "A = np.array([0, 0.5, 1])\n",
    "S = 1000 * np.exp(0.2 * np.arange(-10, 7))\n",
    "\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "eps = 0.01\n",
    "\n",
    "def epsilon():\n",
    "    return random.random() * 2.05 - 0.85\n",
    "\n",
    "def snap_idx(v):\n",
    "    return np.argmin(np.abs(S - v))\n",
    "\n",
    "num_episodes = 20000\n",
    "\n",
    "Q = np.zeros((len(S), len(A)))\n",
    "\n",
    "# Q-learning loop\n",
    "for episode in range(num_episodes):\n",
    "    state_idx = snap_idx(1000) # starting at w = 1000\n",
    "    done = False\n",
    "    t = 0\n",
    "\n",
    "    while not done and t < 40:\n",
    "        if random.random() < eps:\n",
    "            action_idx = np.random.choice(len(A))\n",
    "        else:\n",
    "            action_idx = np.argmax(Q[state_idx])\n",
    "        \n",
    "        action = A[action_idx]\n",
    "\n",
    "        next_w = S[state_idx] * (1 + epsilon() * action)\n",
    "        next_idx = snap_idx(next_w)\n",
    "\n",
    "        if next_w > 3000 or next_w < 150 or t == 39:\n",
    "            reward = next_w\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0\n",
    "        \n",
    "        Q[state_idx, action_idx] += alpha * (reward + gamma * np.max(Q[next_idx, :]) - Q[state_idx, action_idx])\n",
    "\n",
    "        state_idx = next_idx\n",
    "        t += 1\n",
    "\n",
    "pi = A[np.argmax(Q, axis=1)]\n",
    "\n",
    "returns = []\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    state_idx = snap_idx(1000)\n",
    "    done = False\n",
    "    t = 0\n",
    "\n",
    "    while not done and t < 40:\n",
    "        action_idx = np.argmax(Q[state_idx, :])\n",
    "        action = A[action_idx]\n",
    "        next_w = S[state_idx] * (1 + epsilon() * action)\n",
    "        next_idx = snap_idx(next_w)\n",
    "\n",
    "        if next_w > 3000 or next_w < 150 or t == 39:\n",
    "            returns.append(next_w)\n",
    "            done = True\n",
    "        \n",
    "        state_idx = next_idx\n",
    "        t += 1\n",
    "\n",
    "mean = np.mean(returns)\n",
    "stddev = np.std(returns, ddof=1)\n",
    "\n",
    "se = stddev / np.sqrt(len(returns))\n",
    "\n",
    "ci = st.t.interval(0.95, len(returns) - 1, loc=mean, scale=se)\n",
    "\n",
    "\n",
    "print(Q)\n",
    "print(pi)\n",
    "print(\"E(G_0) =\", mean)\n",
    "print(\"95% Confidence Interval:\", ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE results are {1: 1.3547482659740444, 3: 1.335928039470042, 5: 1.0870489498950773, 9: 1.1705002054958564}\n",
      "The bias squared results are {1: 0.4254831045439155, 3: 0.3585949725039857, 5: 0.046714891175322515, 9: 0.0007846149925286416}\n",
      "The variance results are {1: 0.031042874461295816, 3: 0.06082423689995382, 5: 0.0715621130526097, 9: 0.1688548080060313}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "n_train = 100\n",
    "n_test = 1000\n",
    "degrees = [1, 3, 5, 9]\n",
    "n_sims = 100\n",
    "\n",
    "def g(x):\n",
    "    return np.sin(2 * np.pi * x)\n",
    "\n",
    "X_test = np.random.uniform(-1, 1, n_test).reshape(-1, 1)\n",
    "Y_test = g(X_test) + np.random.normal(0, 1, size=X_test.shape)\n",
    "\n",
    "mse_results = {}\n",
    "bias_squared_results = {}\n",
    "variance_results = {}\n",
    "\n",
    "for d in degrees:\n",
    "    f_hat_values = np.zeros((n_sims, n_test))\n",
    "\n",
    "    for sim in range(n_sims):\n",
    "        X_train = np.random.uniform(-1, 1, n_train).reshape(-1, 1)\n",
    "        Y_train = g(X_train) + np.random.normal(0, 1, size=X_train.shape)\n",
    "\n",
    "        poly = PolynomialFeatures(degree=d)\n",
    "        X_train_poly = poly.fit_transform(X_train)\n",
    "        X_test_poly = poly.transform(X_test)\n",
    "\n",
    "        model = LinearRegression().fit(X_train_poly, Y_train)\n",
    "        f_hat_values[sim, :] = model.predict(X_test_poly).flatten()\n",
    "    \n",
    "    f_star = np.mean(f_hat_values, axis=0)\n",
    "\n",
    "    mse = np.mean((f_hat_values - Y_test.T) ** 2)\n",
    "    bias_squared = np.mean((f_star - g(X_test).flatten()) ** 2)\n",
    "    variance = np.mean(np.var(f_hat_values, axis=0))\n",
    "\n",
    "    mse_results[d] = mse\n",
    "    bias_squared_results[d] = bias_squared\n",
    "    variance_results[d] = variance\n",
    "\n",
    "print(f\"The MSE results are {mse_results}\")\n",
    "print(f\"The bias squared results are {bias_squared_results}\")\n",
    "print(f\"The variance results are {variance_results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
